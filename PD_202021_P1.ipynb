{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "PD_202021_P1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamemc/PD_01/blob/main/PD_202021_P1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDNH3eJgN0TE"
      },
      "source": [
        "# Data Mining / Prospeção de Dados\n",
        "\n",
        "## Diogo Soares and Sara C. Madeira, 2020/21\n",
        "\n",
        "# Project 1 - Pattern Mining"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9uj2jIqN0TM"
      },
      "source": [
        "## Logistics \n",
        "**_Read Carefully_**\n",
        "\n",
        "**Students should work in teams of 2 or 3 people**. \n",
        "\n",
        "**TASK 3 - Spring vs Summer Purchases** must be done only by groups of 3 people.\n",
        "\n",
        "Individual projects might be allowed (with valid justification), but will not have better grades for this reason. \n",
        "\n",
        "The quality of the project will dictate its grade, not the number of people working.\n",
        "\n",
        "**The project's solution should be uploaded in Moodle before the end of `March, 28th (23:59)`.** \n",
        "\n",
        "Students should **upload a `.zip` file** containing all the files necessary for project evaluation. \n",
        "Groups should be registered in [Moodle](https://moodle.ciencias.ulisboa.pt/mod/groupselect/view.php?id=139096) and the zip file should be identified as `PDnn.zip` where `nn` is the number of your group.\n",
        "\n",
        "**It is mandatory to produce a Jupyter notebook containing code and text/images/tables/etc describing the solution and the results. Projects not delivered in this format will not be graded. You can use `PD_202021_P1.ipynb`as template. In your `.zip` folder you should also include an HTML version of your notebook with all the outputs** (File > Download as > HTML).\n",
        "\n",
        "**Decisions should be justified and results should be critically discussed.** \n",
        "\n",
        "_Project solutions containing only code and outputs without discussions will achieve a maximum grade 10 out of 20._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDY13YwuN0TN"
      },
      "source": [
        "## Dataset and Tools\n",
        "\n",
        "\n",
        "\n",
        "In this project you will analyse data from an online Store collected over 4 months (April - July 2014). The folder `data` contains three files that you should use to obtain the dataset to be used in pattern mining. \n",
        "\n",
        "The file `store-buys.dat` comprises the buy events of the users over the items. It contains **318.444 sessions**. Each record/line in the file has the following fields (with this order): \n",
        "\n",
        "* **Session ID** - the id of the session. In one session there are one or many buying events. Could be represented as an integer number.\n",
        "* **Timestamp** - the time when the buy occurred. Format of YYYY-MM-DDThh:mm:ss.SSSZ\n",
        "* **Item ID** – the unique identifier of item that has been bought. Could be represented as an integer number. \n",
        "* **Price** – the price of the item. Could be represented as an integer number.\n",
        "* **Quantity** – the quantity in this buying.  Could be represented as an integer number.\n",
        "\n",
        "The file `store-clicks.dat` comprises the clicks of the users over the items. It contains **5.613.499 sessions**.  Each record/line in the file has the following fields (with this order):\n",
        "\n",
        "* **Session ID** – the id of the session. In one session there are one or many clicks. Could be represented as an integer number.\n",
        "* **Timestamp** – the time when the click occurred. Format of YYYY-MM-DDThh:mm:ss.SSSZ\n",
        "* **Item ID** – the unique identifier of the item that has been clicked. Could be represented as an integer number.\n",
        "* **Context** – the context of the click. The value \"S\" indicates a special offer, \"0\" indicates  a missing value, a number between 1 to 12 indicates a real category identifier,\n",
        "any other number indicates a brand. E.g. if an item has been clicked in the context of a promotion or special offer then the value will be \"S\", if the context was a brand i.e BOSCH,\n",
        "then the value will be an 8-10 digits number. If the item has been clicked under regular category, i.e. sport, then the value will be a number between 1 to 12. \n",
        " \n",
        "The file `products.csv` comprises the list of products sold by the online store. It contains **46.294 different products** associated with **123 different subcategories**. Each record/line in the file has the following fields:\n",
        "\n",
        "* **Item ID** - the unique identifier of the item. Could be represented as an integer number. \n",
        "* **Product Categories** - the category and subcategories of the item. It is a string containing the category and subcategories of the item. Eg. `appliances.kitchen.juice`\n",
        "\n",
        "\n",
        "In this project you should use [Python 3](https://www.python.org), [Jupyter Notebook](http://jupyter.org) and **[MLxtend](http://rasbt.github.io/mlxtend/)**. When using MLxtend, frequent patterns can either be discovered using `Apriori` and `FP-Growth`. **Choose the pattern mining algorithm to be used.** \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cM_QHhWnN0TO"
      },
      "source": [
        "## Team Identification\n",
        "\n",
        "**GROUP PD03**\n",
        "\n",
        "Students:\n",
        "\n",
        "* Eduardo Carvalho - nº55881\n",
        "* Filipe Santos - nº55142\n",
        "* Ivo Oliveira - nº50301"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxjFy6emN0TP"
      },
      "source": [
        "## 1. Mining Frequent Itemsets and Association Rules\n",
        "\n",
        "\n",
        "In this first part of the project you should load and preprocess the dataset  in order to compute frequent itemsets and generate association rules considering all the sessions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vg_C5lWnN0TQ"
      },
      "source": [
        "**In what follows keep the following question in mind and be creative!**\n",
        "\n",
        "1. What are the most interesting products?\n",
        "2. What are the most bought products?\n",
        "3. Which products are bought together?\n",
        "4. Can you find associations between the clicked products? \n",
        "5. Can you find associations highliting that when people buy a product/set of products also buy other product(s)?\n",
        "6. Can you find associations highliting that when people click in a product/set of products also buy this product(s)?\n",
        "7. Can you find relevant associated categories? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sip5BTVN0TQ"
      },
      "source": [
        "### 1.1. Load and Preprocess Data\n",
        "\n",
        " **Product quantities were not be considered.**\n",
        "\n",
        "##Limitations\n",
        "*Due to a problem in the database quantity (QTY) column, the SessionID that corresponds to an equivalent ID in Buys, was thus considered as a purchase. Altough this skews the expected results, the fact some buys had quantities of 0 and other problems, this was a temporarly, yet accurate metric of products and their given clicks and purchases*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B91Kkqn7UPtp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "007c9813-8d9f-4af3-8d85-96ea39dac895"
      },
      "source": [
        "#!pip install mlxtend\n",
        "!pip install mlxtend --upgrade\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.collections import QuadMesh\n",
        "from matplotlib.text import Text\n",
        "from matplotlib.colors import LogNorm\n",
        "import seaborn as sns\n",
        "import tkinter as tkr\n",
        "from mlxtend.preprocessing import  TransactionEncoder\n",
        "from mlxtend.frequent_patterns import apriori\n",
        "from mlxtend.frequent_patterns import association_rules\n",
        "from mlxtend.frequent_patterns import fpgrowth"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: mlxtend in /usr/local/lib/python3.7/dist-packages (0.18.0)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from mlxtend) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from mlxtend) (54.1.2)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.13.2 in /usr/local/lib/python3.7/dist-packages (from mlxtend) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn>=0.20.3 in /usr/local/lib/python3.7/dist-packages (from mlxtend) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.16.2 in /usr/local/lib/python3.7/dist-packages (from mlxtend) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: pandas>=0.24.2 in /usr/local/lib/python3.7/dist-packages (from mlxtend) (1.1.5)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from mlxtend) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->mlxtend) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->mlxtend) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->mlxtend) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->mlxtend) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.2->mlxtend) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib>=3.0.0->mlxtend) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNqxZ80XWZv_",
        "outputId": "29884b9f-4a0c-4f3c-b065-4dd6eee3bb66"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKIFWC5aWfje"
      },
      "source": [
        "products=pd.read_csv('/content/drive/MyDrive/Datasets/projetoPD/products.csv',\n",
        "                     header=None,\n",
        "                     names=['ItemID','Category'],\n",
        "                     dtype={'ItemID': np.int32, 'Category':str})\n",
        "buys=pd.read_csv('/content/drive/MyDrive/Datasets/projetoPD/store-buys.dat', \n",
        "                 header=None,\n",
        "                 names=['SessionID','TimeStamp','ItemID','Price','Qty'],\n",
        "                 dtype={'SessionID':np.int32, 'TimeStamp':str, 'ItemID': np.int32, \n",
        "                        'Price':np.int16, 'Qty':'category'})\n",
        "clicks=pd.read_csv('/content/drive/MyDrive/Datasets/projetoPD/store-clicks.dat', \n",
        "                   header=None,\n",
        "                   nrows=1000000,\n",
        "                   names=['SessionID','TimeStamp','ItemID','Context'],\n",
        "                   dtype={'SessionID':np.int32, 'TimeStamp':str, 'ItemID': np.int32, \n",
        "                        'Context':'category'})"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lb-0bAsvxKLF",
        "outputId": "599f2d59-1cf0-40bd-f547-9c2cf2383148",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "clicks.info()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000000 entries, 0 to 999999\n",
            "Data columns (total 4 columns):\n",
            " #   Column     Non-Null Count    Dtype   \n",
            "---  ------     --------------    -----   \n",
            " 0   SessionID  1000000 non-null  int32   \n",
            " 1   TimeStamp  1000000 non-null  object  \n",
            " 2   ItemID     1000000 non-null  int32   \n",
            " 3   Context    1000000 non-null  category\n",
            "dtypes: category(1), int32(2), object(1)\n",
            "memory usage: 16.2+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWWws1aeWWHv"
      },
      "source": [
        "#Database Corrections\n",
        "This code segment contains Database corrections of the following:\n",
        "- products_no_duplicates - removed duplicates\n",
        "- buys_upd - added product names & season"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTz2Y-u09rbA"
      },
      "source": [
        "\"\"\"\n",
        "DATABASE CORRECTIONS\n",
        "products_no_duplicates - removed duplicates\n",
        "buys_upd - added product names & season\n",
        "\"\"\"\n",
        "buys = buys.drop(columns=['Qty'])\n",
        "products_no_duplicates = products.drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "buys_upd =pd.merge(buys,products_no_duplicates)\n",
        "clicks_upd= pd.merge(clicks,products_no_duplicates)\n",
        "\n",
        "#ignore prices/quantities of 0\n",
        "#buys_upd = buys_upd[buys_upd.Price>0]\n",
        "#buys_upd = buys_upd[buys_upd.Qty>0]\n",
        "#no click_upd ver context=0 (missing value)\n",
        "\n",
        "product_name_buys=[]\n",
        "for cat in buys_upd.Category:\n",
        "  product_name_buys.append(cat.split('.')[-1].replace('_',' ').title())\n",
        "product_name_clicks=[]\n",
        "for cat in clicks_upd.Category:\n",
        "  product_name_clicks.append(cat.split('.')[-1].replace('_',' ').title())\n",
        "#product_name[:5]\n",
        "\n",
        "buys_upd['ProductName']= product_name_buys\n",
        "buys_upd = buys_upd.drop('Category', axis=1)\n",
        "clicks_upd['ProductName']=product_name_clicks\n",
        "clicks_upd=clicks_upd.drop('Category', axis=1)\n",
        "\n",
        "dates_buys =[]\n",
        "season_buys =[]\n",
        "dates_clicks =[]\n",
        "season_clicks =[]\n",
        "\n",
        "for i in buys_upd[\"TimeStamp\"]:\n",
        "  dates_buys.append(i[:10])\n",
        "  if i[5:7] == \"04\" or i[5:7] == \"05\":\n",
        "     season_buys.append(\"Spring\")\n",
        "  elif i[5:7] == \"06\" or i[5:7] == \"07\":\n",
        "    season_buys.append(\"Summer\")\n",
        "  else:\n",
        "    season_buys.append(\"Other\")\n",
        "\n",
        "for i in clicks_upd[\"TimeStamp\"]:\n",
        "  dates_clicks.append(i[:10])\n",
        "  if i[5:7] == \"04\" or i[5:7] == \"05\":\n",
        "     season_clicks.append(\"Spring\")\n",
        "  elif i[5:7] == \"06\" or i[5:7] == \"07\":\n",
        "    season_clicks.append(\"Summer\")\n",
        "  else:\n",
        "    season_clicks.append(\"Other\")\n",
        "\n",
        "weekday_buys=[]\n",
        "buys_upd[\"TimeStamp\"] = pd.to_datetime(buys_upd[\"TimeStamp\"])\n",
        "buys_upd[\"Weekday_Num\"]=buys_upd[\"TimeStamp\"].dt.dayofweek \n",
        "for i in buys_upd[\"Weekday_Num\"]:\n",
        "  if i < 5: \n",
        "    weekday_buys.append(\"Weekday\")\n",
        "  else:\n",
        "    weekday_buys.append(\"Weekend\")\n",
        "\n",
        "weekday_clicks=[]\n",
        "clicks_upd[\"TimeStamp\"] = pd.to_datetime(clicks_upd[\"TimeStamp\"])\n",
        "clicks_upd[\"Weekday_Num\"]=clicks_upd[\"TimeStamp\"].dt.dayofweek \n",
        "for i in clicks_upd[\"Weekday_Num\"]:\n",
        "  if i < 5: \n",
        "    weekday_clicks.append(\"Weekday\")\n",
        "  else :\n",
        "    weekday_clicks.append(\"Weekend\")\n",
        "  \n",
        "buys_upd = buys_upd.drop(columns=['TimeStamp'])\n",
        "buys_upd.insert(1, \"TimeStamp\", dates_buys)\n",
        "buys_upd[\"Season\"] = season_buys\n",
        "buys_upd[\"Weekday\"] = weekday_buys\n",
        "buys_upd.sort_values(by='TimeStamp')\n",
        "\n",
        "clicks_upd = clicks_upd.drop(columns=['TimeStamp'])\n",
        "clicks_upd.insert(1, \"TimeStamp\", dates_clicks)\n",
        "clicks_upd[\"Season\"] = season_clicks\n",
        "clicks_upd[\"Weekday\"] = weekday_clicks\n",
        "clicks_upd.sort_values(by='TimeStamp')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWXryR7jXqam"
      },
      "source": [
        "buys_upd=buys_upd.sort_values(by='SessionID').reset_index(drop=True)\n",
        "buys_upd.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rwLtqj_iI2L"
      },
      "source": [
        "Existem 113 registos duplicados. Precisamos de os remover para avançar com a análise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HU_U5TpgiU4m"
      },
      "source": [
        "buys_upd=buys_upd.drop_duplicates().reset_index(drop=True)\n",
        "len(buys_upd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDezmzROYVeL"
      },
      "source": [
        "clicks_upd=clicks_upd.sort_values(by='SessionID').reset_index(drop=True)\n",
        "clicks_upd.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "473WzJVeii2-"
      },
      "source": [
        "len(clicks)-len(clicks.drop_duplicates())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vFbpvpji3FI"
      },
      "source": [
        "Existem 46 registos duplicados. Precisamos de os remover para avançar com a análise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoS4GXLUi5KN"
      },
      "source": [
        "clicks_upd=clicks_upd.drop_duplicates().reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxZsNNFdWtju"
      },
      "source": [
        "# Most interesting items?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKUy8O4mXFvg"
      },
      "source": [
        "## Most Clicked Products\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzTwF_zTUqty"
      },
      "source": [
        "# What are the most interesting items? (most clicked)\n",
        "most_clicks = clicks_upd.groupby([\"ProductName\"], as_index=False).count().sort_values(by=\"SessionID\", ascending=False)\n",
        "most_clicks = most_clicks[['ProductName','SessionID']].reset_index(drop=True)\n",
        "most_clicks = most_clicks.rename(columns={'SessionID':'Clicks'})\n",
        "most_clicks.index += 1\n",
        "print(\"Table 1.\")\n",
        "most_clicks.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCbsL2JnXUfB"
      },
      "source": [
        "> *The top three most clicked products are Memory Chips, with a total of 71265, followed by Blenders with 56013 and lastly, Meat Grinders with 32859. Below the product purchases will be shown through a a bar graph.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6YbKXJiMkFk"
      },
      "source": [
        "#2. What are the most bought products?\n",
        "quantities = buys_upd.groupby([\"ProductName\"], as_index=False).count().sort_values(by=\"SessionID\", ascending=False)\n",
        "quantities2 = quantities\n",
        "quantities = quantities[['ProductName','SessionID']].reset_index(drop=True)\n",
        "quantities = quantities.rename(columns={'SessionID':'SalesCount'})\n",
        "quantities.index += 1 \n",
        "\n",
        "quantities = quantities[quantities.SalesCount>5000]\n",
        "most_bought = sns.catplot(x=\"ProductName\", y=\"SalesCount\", kind=\"bar\", data = quantities)\n",
        "most_bought.fig.suptitle('Figure 1. Number of Sales per Product')\n",
        "for ax in most_bought.axes.flat:\n",
        "    for label in ax.get_xticklabels():\n",
        "        label.set_rotation(75)\n",
        "most_bought.set(yticks=np.arange(0,60000,2500))\n",
        "most_bought.fig.set_figheight(12.5)\n",
        "most_bought.fig.set_figwidth(25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztui73-RaN1f"
      },
      "source": [
        "> *The bar graph above, clearly shows the differences between product purchases. Interestingly products with more than 5 thousand sales don't exactly match to the most clicked items as seen in the previous table (Table 1.).\n",
        "For example, more than 55 thousand Tennis products were sold, however the same product can't be found in the top ten of most clicked online.\n",
        "The same happens in Lawn Mowers, with around 32.5 thousand sales.\n",
        "Memory chips, the second most bought has 32.5 thousand sales, that's 45% of the number of respective product clicks of said product.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKvW8ziSe0NB"
      },
      "source": [
        "#prepare graph info\n",
        "quantities2 = buys_upd\n",
        "quantities2['SessionID'] = 1\n",
        "daysofWeek={0:'Segunda-Feira',1:'Terça-Feira',2:'Quarta-Feira',3:'Quinta-Feira', 4:'Sexta-Feira',5:'Sábado',6:'Domingo'}\n",
        "quantities2['Weekday_Num'] = quantities2['Weekday_Num'].map(daysofWeek)\n",
        "quantities2 = quantities2.groupby([\"Weekday_Num\",\"ProductName\"], as_index=False)['SessionID'].sum()\n",
        "\n",
        "drop_column = quantities2.groupby(['ProductName'], as_index=False)['SessionID'].sum()\n",
        "drop_column = drop_column.sort_values(by='SessionID', ascending=False)\n",
        "drop_column = drop_column.tail(-20)\n",
        "drop_column = drop_column['ProductName']\n",
        "\n",
        "quantities2 = quantities2[~quantities2['ProductName'].isin(drop_column)]\n",
        "quantities2 = quantities2.sort_values(by='SessionID', ascending=False).reset_index(drop=True)\n",
        "quantities2 = quantities2.pivot_table(index='Weekday_Num', columns = 'ProductName')['SessionID']\n",
        "quantities2 = quantities2.reindex(['Segunda-Feira','Terça-Feira', 'Quarta-Feira','Quinta-Feira', 'Sexta-Feira','Sábado','Domingo'])\n",
        "# ----- get max & min values for graph\n",
        "hm_vmax = quantities2.max()\n",
        "hm_vmax = hm_vmax.max()\n",
        "hm_vmin = quantities2.min()\n",
        "hm_vmin = hm_vmin.min()\n",
        "quantities2['Total'] = quantities2.sum(axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYmkJddUBDvj"
      },
      "source": [
        "#process data for heatmap graph\n",
        "sns.set(font_scale = 1.4)\n",
        "fig, ax = plt.subplots(figsize=(30,10))\n",
        "plt.suptitle(\"Product clicks per day of the week\")\n",
        "sns.heatmap(quantities2, cmap='coolwarm', square=True, linewidths=1, annot=True, fmt=\"d\", vmin=hm_vmin*2, vmax=hm_vmax/2.5)\n",
        "quadmesh = ax.findobj(QuadMesh)[0] # get array of colors\n",
        "facecolors = quadmesh.get_facecolors()\n",
        "facecolors[np.arange(20,147,21)] = np.array([1,1,1,1]) # make colors of the last column white\n",
        "quadmesh.set_facecolors = facecolors # set modified colors\n",
        "for i in ax.findobj(Text): # set color of all text to black\n",
        "    i.set_color('black')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acR9RFBotBPA"
      },
      "source": [
        "> *Above is an heat map analysis of the 20 most sold products (sorted alphabetically) and their sales per day of the week. This visualization goes on pair with the previous bar graph, and in it we can see the sales dispersion clearly. On the rightmost column is the total number of sales per day.*\n",
        "\n",
        ">> *Sunday is by far the most popular day, with a total of 109025 sales and with the cells with the higher numbers. Tennis products bought at Sundays which number 16183 are the combination with the higher count.*\n",
        "\n",
        ">> *Besides Sundays, Saturdays and Mondays are the most popular days of the week to buy online with 62383 and 74115 purchases, respectively. On the opposite end, Tuesday is is the least popular day, with only 19855 purchases.*\n",
        "\n",
        ">> *As shown before, Tennis products are the most popular, and this is true for any day of the week compared to other products within the same day.*\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGtqXH2AN0TR"
      },
      "source": [
        "## 1.2. Compute Frequent Itemsets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlQvTGPlN0TR"
      },
      "source": [
        "* Compute frequent itemsets considering a minimum support of X%. \n",
        "* Present frequent itemsets organized by length (number of items). \n",
        "* List frequent 1-itemsets, 2-itemsets, 3-itemsets, etc with support of at least Y%.\n",
        "* Change X and Y when it makes sense and discuss the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C75zFtKn0PjW"
      },
      "source": [
        "#Setting up: Which products are bought together?\n",
        "all_sessions_buy={}\n",
        "for i in range(len(buys_upd)):\n",
        "  all_sessions_buy[buys_upd.SessionID[i]]=[]\n",
        "\n",
        "for i in range(len(buys_upd)):\n",
        "  if buys_upd.ProductName[i] not in all_sessions_buy.get(buys_upd.SessionID[i]):\n",
        "    all_sessions_buy[buys_upd.SessionID[i]].append(buys_upd.ProductName[i])\n",
        "\n",
        "transactions_buy=list(all_sessions_buy.values())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxUGXN9IOkts"
      },
      "source": [
        "#Setting up: Which products are viewed together?\n",
        "all_sessions_click={}\n",
        "for i in range(len(clicks_upd)):\n",
        "  all_sessions_click[clicks_upd.SessionID[i]]=[]\n",
        "\n",
        "for i in range(len(clicks_upd)):\n",
        "  if clicks_upd.ProductName[i] not in all_sessions_click.get(clicks_upd.SessionID[i]):\n",
        "    all_sessions_click[clicks_upd.SessionID[i]].append(clicks_upd.ProductName[i])\n",
        "\n",
        "transactions_click=list(all_sessions_click.values())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SM-xjtM0D0DV"
      },
      "source": [
        "#Compute binary databases\n",
        "tr_enc = TransactionEncoder()\n",
        "\n",
        "#buys\n",
        "trans_array_buy = tr_enc.fit(transactions_buy).transform(transactions_buy)\n",
        "binary_database_buy = pd.DataFrame(trans_array_buy, columns=tr_enc.columns_)\n",
        "binary_database_buy.head(10)\n",
        "#clicks\n",
        "trans_array_click = tr_enc.fit(transactions_click).transform(transactions_click)\n",
        "binary_database_click = pd.DataFrame(trans_array_click, columns=tr_enc.columns_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ewf1HtdENP_"
      },
      "source": [
        "#Compute itemsets min_support = 1% apriori and association rules\n",
        "\n",
        "#buys\n",
        "frequent_itemsets_buy = apriori(binary_database_buy, min_support=0.01, use_colnames=True, low_memory=True)\n",
        "frequent_itemsets_buy\n",
        "#Rules grau 1\n",
        "rules1_buy = association_rules(frequent_itemsets_buy, metric=\"confidence\", min_threshold=0.01)\n",
        "#rules1_buy\n",
        "# add new column length\n",
        "frequent_itemsets_buy['length'] = frequent_itemsets_buy['itemsets'].apply(lambda x: len(x))\n",
        "# filter using pattern length = 2\n",
        "frequent_2_itemsets_buy = frequent_itemsets_buy[frequent_itemsets_buy['length'] >= 2].reset_index(drop=True)\n",
        "#frequent_2_itemsets_buy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1U9VS9xflgqA"
      },
      "source": [
        "#clicks\n",
        "frequent_itemsets_click = apriori(binary_database_click, min_support=0.01, use_colnames=True, low_memory=True)\n",
        "frequent_itemsets_click\n",
        "#Rules grau 1\n",
        "rules1_click = association_rules(frequent_itemsets_click, metric=\"confidence\", min_threshold=0.01)\n",
        "#rules1_click\n",
        "# add new column length\n",
        "frequent_itemsets_click['length'] = frequent_itemsets_click['itemsets'].apply(lambda x: len(x))\n",
        "# filter using pattern length = 2\n",
        "frequent_2_itemsets_click = frequent_itemsets_click[frequent_itemsets_click['length'] >= 2].reset_index(drop=True)\n",
        "#frequent_2_itemsets_click"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhIdLvIxi4Sb"
      },
      "source": [
        "#FP-Growth é melhori que apriori\n",
        "#buys\n",
        "frequent_itemsets_fpg_buy=fpgrowth(binary_database_buy, min_support=0.01,use_colnames=True, low_memory=True)\n",
        "frequent_itemsets_fpg_buy\n",
        "# Generate association rules with confidence >= 90%\n",
        "\n",
        "rules_buy = association_rules(frequent_itemsets_fpg_buy, metric = \"confidence\", min_threshold=0.01)\n",
        "rules_buy\n",
        "#clicks\n",
        "frequent_itemsets_fpg_click=fpgrowth(binary_database_click, min_support=0.01,use_colnames=True, low_memory=True)\n",
        "frequent_itemsets_fpg_click\n",
        "# Generate association rules with confidence >= 90%\n",
        "\n",
        "rules_click = association_rules(frequent_itemsets_fpg_click, metric = \"confidence\", min_threshold=0.01)\n",
        "rules_click"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xni6Rit9N0TS"
      },
      "source": [
        "### 1.3. Generate Association Rules from Frequent Itemsets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfdeOt6uN0TT"
      },
      "source": [
        "* Generate association rules with a choosed value (C) for minimum confidence. \n",
        "* Generate association rules with a choosed value (L) for minimum lift. \n",
        "* Generate association rules with both confidence >= C% and lift >= L.\n",
        "* Change C and L when it makes sense and discuss the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "611Q1RXeN0TU"
      },
      "source": [
        "### 1.4. Take a Look at Maximal Patterns: Compute Maximal Frequent Itemsets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFKZRaiNN0TV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCjPFDkhN0TW"
      },
      "source": [
        "### 1.5. Conclusions "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wckLxbQ5N0TW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8AjOWwaN0TW"
      },
      "source": [
        "# 2. Week vs Weekend Purchases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNinPpoDN0TX"
      },
      "source": [
        "In this part of the project you should analyse the consumption patterns during the week vs during the weekeed.\n",
        "\n",
        "**In what follows keep the following question in mind and be creative!**\n",
        "\n",
        "1. The most interesting products are the same during the week and the weekend? \n",
        "2. What are the most bought products during the week? And during the weekend?\n",
        "3. There are differences between the sets of products bought during the week and the weekend?\n",
        "4. Can you find different associations highliting that when people click in a product/set of products also buy this product(s) during the week vs the weekend?\n",
        "5. Discuss the results obtained for the week sessions vs weekend sessions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rqmzoYiN0TX"
      },
      "source": [
        "### 2.1. Load and Preprocess Data\n",
        "\n",
        " **Product quantities should not be considered.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUFNubG3N0TY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQPHNWJiN0TY"
      },
      "source": [
        "### 2.2. Compute Frequent Itemsets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36ZKu4VfN0TY"
      },
      "source": [
        "* Compute frequent itemsets considering a minimum support of X%. \n",
        "* Present frequent itemsets organized by length (number of items). \n",
        "* List frequent 1-itemsets, 2-itemsets, 3-itemsets, etc with support of at least Y%.\n",
        "* Change X and Y when it makes sense and discuss the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmX6B6fNN0TY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZrOgnotN0TY"
      },
      "source": [
        "### 2.3. Generate Association Rules from Frequent Itemsets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPxat6z9N0TY"
      },
      "source": [
        "* Generate association rules with a choosed value (C) for minimum confidence. \n",
        "* Generate association rules with a choosed value (L) for minimum lift. \n",
        "* Generate association rules with both confidence >= C% and lift >= L.\n",
        "* Change C and L when it makes sense and discuss the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wK5yk2o6N0TZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NpUVD7RN0Ta"
      },
      "source": [
        "### 2.4. Conclusions "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dq4kYJZ2N0Ta"
      },
      "source": [
        "# 3. [Only Groups of 3] Spring vs Summer Purchases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9p1BNhLN0Ta"
      },
      "source": [
        "In this part of the project you should analyse the consumption patterns during the Spring months (April and May) vs Summer months (June and July).\n",
        "\n",
        "**In what follows keep the following question in mind and be creative!**\n",
        "\n",
        "1. The most interesting products are the same during the Spring and the Summer? \n",
        "2. What are the most bought products during the Spring? And during the Summer?\n",
        "3. There are differences between the sets of products bought during the Spring and the Summer?\n",
        "4. Can you find different associations highliting that when people click in a product/set of products also buy this product(s) during the Spring vs the Summer?\n",
        "5. Discuss the results obtained for the Spring sessions vs Summer sessions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_dXMFZEN0Ta"
      },
      "source": [
        "### 3.1. Load and Preprocess Data\n",
        "\n",
        " **Product quantities should not be considered.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-Y06iJAN0Ta"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bU3Pkq81N0Tb"
      },
      "source": [
        "### 3.2. Compute Frequent Itemsets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFfigBw4N0Tb"
      },
      "source": [
        "* Compute frequent itemsets considering a minimum support of X%. \n",
        "* Present frequent itemsets organized by length (number of items). \n",
        "* List frequent 1-itemsets, 2-itemsets, 3-itemsets, etc with support of at least Y%.\n",
        "* Change X and Y when it makes sense and discuss the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EFKQA3uN0Tb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7-5bAz4N0Tb"
      },
      "source": [
        "### 3.3. Generate Association Rules from Frequent Itemsets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phBJzDRsN0Tc"
      },
      "source": [
        "* Generate association rules with a choosed value (C) for minimum confidence. \n",
        "* Generate association rules with a choosed value (L) for minimum lift. \n",
        "* Generate association rules with both confidence >= C% and lift >= L.\n",
        "* Change C and L when it makes sense and discuss the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emPKRX6UN0Tc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1Ep7ZlbN0Tc"
      },
      "source": [
        "### 3.4. Conclusions "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huX6JwLPN0Tc"
      },
      "source": [
        "## 4. Conclusions\n",
        "Draw some conclusions about this project work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTELnhtnN0Td"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}